# Automated Profile Inference with Language Model Agents

This repository contains the implementation of **AutoProfiler** on the synthetic dataset, a novel multi-agent system that performs automated profile inference using Large Language Models (LLMs). 

## ğŸ¯ Overview

AutoProfiler combines:
- **Multi-agent coordination** for systematic information extraction
- **Retrieval-Augmented Generation (RAG)** for context-aware reasoning (optional)
- **Iterative refinement** through agent interactions
- **Confidence-based filtering** for reliable attribute inference

The system can infer various personal attributes including age, education, location, occupation, relationship status, income level, and more from user-generated text content.

## ğŸ—ï¸ Architecture

### Multi-Agent System Design

The system employs four specialized agents that work in coordination:

1. **Retriever Agent**
   - Responsible for gathering relevant user history and context
   - Implements semantic search and chronological retrieval
   - Provides targeted information to the Extractor agent

2. **Strategist/Extractor Agent**
   - Coordinate the attack plan
   - Core reasoning agent that analyzes user content
   - Performs iterative attribute inference with confidence scoring

3. **Summarizer Agent**
   - Validates and consolidates inferred attributes
   - Removes duplicates and conflicting information
   - Generates natural language summaries of results

Note that in implementation we combine Strategist and Extractor as Profiler.

### Key Components

```
AutoProfiler/
â”œâ”€â”€ agents/           # Multi-agent implementation
â”‚   â”œâ”€â”€ retriever.py  # Information retrieval agent
â”‚   â”œâ”€â”€ profiler.py   # Core plan/inference agent
â”‚   â””â”€â”€ summarizer.py # Result validation agent
â”œâ”€â”€ config/           # Model and API configurations
â”œâ”€â”€ dataset/          # SynthPAI dataset and results
â”œâ”€â”€ functions/        # Local retrieval functions
â”œâ”€â”€ prompts/          # Agent-specific prompts
â”œâ”€â”€ util/             # Data loading and utility functions
â””â”€â”€ main.py          # Main execution script
```

## ğŸ“Š Dataset

### Dataset Structure

```
dataset/
â”œâ”€â”€ synthpai/           # User history files
â”‚   â”œâ”€â”€ User1/         # Individual user directories
â”‚   â””â”€â”€ User2/
â”œâ”€â”€ ground_truth.json  # Ground truth annotations
â””â”€â”€ {model}/           # Inference results by model
    â”œâ”€â”€ pii/           # Extracted personal information
    â””â”€â”€ summary/       # Natural language summaries
```

## ğŸš€ Installation

### Prerequisites

- Python 3.8+
- OpenAI API key (or compatible LLM provider)
- Bing Search API key (for web search functionality)

### Setup

1. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

2. **Configure API keys**:
   - Edit `config/gpt-4.json` with your LLM API credentials
   - Edit `config/web_api.py` with your search API keys

## ğŸ’» Usage

### Basic Usage

Run the inference attack on a target user:

```bash
python main.py -m gpt-4 -u UserName
```

### Parameters

- `-m, --llm_model`: LLM model to use (e.g., gpt-4, claude, gemini)
- `-u, --target_user`: Target user identifier from SynthPAI dataset


## ğŸ”’ Ethical Considerations

This research is conducted for **academic purposes only**. The system is designed to:
- **Raise awareness** about privacy risks in online content
- **Improve privacy protection** mechanisms of LLMs
- **Advance the field** of privacy and machine learning


**Disclaimer**: This tool should not be used for unauthorized personal information extraction or privacy violations. This research is for academic purposes only. Users are responsible for complying with all applicable laws and ethical guidelines.

